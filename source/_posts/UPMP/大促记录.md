---
categories:
  - UPMP
---
# 2020.11.01-2020.11.11大促记录

# 1.开门红

**11.01 00:00--11.01 01:00**

## 1. JSF接口单量

![image-20201106144758814](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201106144758814.png)

09分左右达到峰值30w/min。tp99位55ms

## 2. JMQ匹配试用品

![image-20201106145116491](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201106145116491.png)

> 峰值为23w/min,tp99为40ms.。ts_transferAfter放量峰值63.6w/min



> 14551条匹配成功但是因为JMQ积压导致
>
> 凌晨OFC调用量峰值为30w/min,大促预估峰值100w/min。凌晨JMq峰值63.6万/min，积压情况在与转移同事还有Mq支持同事沟通@吕尤



# 2.应对大促，大促预估

1.JSF下传订单预估峰值1.5倍开门红，考虑618峰值为65w/min左右，同时考虑到promise提前放量。以100w/min流量应对

2.JMQ预估75w/min,promise下传提前，预估增加至90w/min

3.Jimdb 使用超80%有淘汰策略风险

> @全部成员 辛苦各个部门主动梳理自己名下的jimdb实例，对集群内存或某分片利用率使用率超过80%的进行评估是否申请扩容或清理数据（因资源紧张，仅针对0级系统才支持扩容）。 1、jimdb实例内存如果被打满100%，因默认的淘汰策略是volatile-lru，会将不稳定的key从内存中移除，被移除的key有可能还未过期，因此存在丢失数据的风险。 2、如果是集群确定为不再使用，请及时销毁。 3、如果统计出的集群还在使用，内存使用率&gt;80%，请及时自行协调资源或者优化数据量，降低丢失数据的风险。避免大促时内存大幅增长影响同宿主机其他应用。

[redis 过期建删除策略](https://www.jianshu.com/p/9352d20fb2e0)

4.UMP监控

> 订单下传：http://ump2.jd.com/monitor/perfomance?appId=122371&appName=upmp2&thresholdOn=0&isSecond=0&frequency=1&endPointKey=Upmp2.UpmpService.getTesterMarketingByJSON&platform=j-one&tab=method

> 订单匹配：http://ump2.jd.com/monitor/perfomance?appId=122371&appName=upmp2&thresholdOn=0&isSecond=0&frequency=1&endPointKey=Upmp.GoldenProcessJmqListener.onMessage&platform=j-one&tab=method

> 订单详情查询：http://ump2.jd.com/monitor/perfomance?appId=122371&appName=upmp2&thresholdOn=0&isSecond=0&frequency=1&endPointKey=Upmp.TrialProductProviderImpl.queryProductByOrderId&platform=j-one&tab=method

> 订单取消：http://ump2.jd.com/monitor/perfomance?appId=122371&appName=upmp2&thresholdOn=0&isSecond=0&frequency=1&endPointKey=Upmp.CancelOrderListener.onMessage&platform=j-one&tab=method

> 订单匹配成功：http://ump2.jd.com/monitor/perfomance?appId=122371&appName=upmp2&thresholdOn=0&isSecond=0&frequency=1&endPointKey=Upmp.GoldenProcessJmqListener.presistOrderRecord&platform=j-one&tab=method

> JMQ消息接收监控：http://ump2.jd.com/monitor/perfomance?endPointKey=jmq-servers.client.consume.upmp2.ts_transferAfter

> 上游JMQ入队埋点：  http://jmq.jd.com/chart/hostList.do?viewCode=60001&topic=ts_transferAfter&app=OrderTransfer&hosts[]=172_20_80_42_50088&hosts[]=172_20_80_86_50088&hosts[]=172_28_64_56_50088&hosts[]=172_18_144_44_50088&hosts[]=11_3_113_138_50088&hosts[]=11_3_113_162_50088&
>
> JMQ出队埋点：   http://jmq.jd.com/chart/hostList.do?viewCode=50001&topic=ts_transferAfter&app=upmp2&hosts[]=172_20_80_42_50088&hosts[]=172_20_80_86_50088&hosts[]=172_28_64_56_50088&hosts[]=172_18_144_44_50088&hosts[]=11_3_113_138_50088&hosts[]=11_3_113_162_50088&



### 策略：

1.申请机器：主要是考虑了量级（JMQ峰值90w/min时满CPU需要15台（每台6w/min）,JSF100W/min时满CPU需17台（每台6w/min），另外落库8w/min需8台左右，*2CPU维持在30%~40%左右）

> ==廊坊==
>
> 11.95.152.133
> 11.55.3.220
> 11.55.250.235
> 11.55.250.232
> 11.55.250.225
> 11.55.250.224
> 11.55.249.56
> 11.55.249.42
> 11.55.249.41
> 11.55.249.39
> 11.55.232.46
> 11.55.209.205
> 11.55.180.7
> 11.52.64.160
> 11.52.64.158
> 11.52.64.157
> 11.52.48.86
> 11.46.115.93
> 11.46.115.85
> 11.46.115.49
>
> ==汇天==
>
> 11.37.72.117
> 11.37.69.96
> 11.37.69.95
> 11.24.92.155
> 11.24.92.154
> 11.24.92.153
> 11.24.49.209
> 11.21.71.175
> 11.21.36.191
> 11.21.13.27
> 11.20.66.212
> 11.20.47.18
> 11.20.47.17
> 11.20.47.16
> 11.20.211.53
> 11.19.89.143
> 11.19.89.142
> 11.19.5.76
> 11.19.213.254
> 11.19.0.198

2. JMQ修改+队列以及消费方式

> JMQ开门红积压量至89w，0.11分时峰值63w/min, 
>
> 消费队列*2，并行开启，单队列最大并行数20，若不考虑性能影响，实际会增加消费4倍。
>
> 开门红当天mq消费峰值22w/min,考虑队列+并行30应该能满足100w/min。但是CPU同时消费JSF任务，会影响JMQ吞吐量，使其消费不足预想。
>
> 又因分片4*队列20 <机器100,会有部分机器空闲。串行消费时每台机器默认sdk线程数为80，此时若有加队列发生，若机器数不足上面所讲80，可能对机器造成宕机风险。我们申请后机器数达100，完成可以串行消费生产的jmq消息，为提高消费能力，可考虑开启并行，由空闲机器拉取jmq消息，消除jmq生产峰值。
>
> **并行消费MQ理解**
>
> 极限情况1台机器会建立分片*队列个线程（TomcatLinux下最大1000线程）取消费消息，所以开10倍并行使用10台机器就可以。
>
> 考虑到复杂生产环境，其他任务争抢，以及CPU处理能力，网络带宽等每台机器处理能力受限，所以多开机器自然重复利用并行消费，去拉取消息然后上锁消费。
>
> 测试单台机器10队列消费jmq串行6W/min,并行消费10队列单台消费10w/min,并行消费20队列单台消费20w/min。在生产环境中消费能力一定下降。

3.Jimdb报警内存不足80%

> Jimdb 增加分片，扩容。
>
> ops 每秒操作数超11w/min报警，CPU使用率超 90%，内存使用超70% 报警

4.j-one机器

> 在CPU使用上不来的情况下，考虑是否是Tomcat的最大线程池受限影响，导致线程未最大程度使用CPU，但是调查发现Linux服务器Tomcat最大线程池为1000。j-one机器也设置为1000，那就是每个线程利用CPU不理想，可单独开启线程池充分利用，或者机器分组单独执行功能，减少CPU调度花费。

# 3.双11

## 1**JMQ匹配试用品**

![image-20201112174213061](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201112174213061.png)

## 2.**JSF匹配话卡**

![image-20201112174317892](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201112174317892.png)

## 3. 总结

**精准投放系统11日首小时：派发中活动42个。0点到1点共处理订单1621万，消息处理峰值53万/分钟，接口峰值40万/分钟，共匹配试用品订单约6万单，其中前10分钟，匹配试用品订单即超过2万。首小时处理单量是开门红的2倍，平日时均的30倍，试用品匹配订单量是开门红的5.7倍，平日时均的25倍。系统平稳，未出现处理积压情况。**

1.JMQ未发生积压说明：

**事先准备：**

> 1.增加broker2个，共6个。增加队列数10个，共20个。
>
> 2.计算公式：
>
> ts_transferAfter主题的出队速度估算： tpAvg为平均消费tp时间
> 分片数 x 队列数 x 并行数量/批量大小 x (1000/tpAvg) * 损耗系数0.9
> 6分片 * 20队列 * 80并行数/10批量 *（1000ms/40ms）*损耗0.9 = 21600  （机器不是瓶颈的前提下，每秒可消费2w条消息）
>
> 3.跟上游沟通要umpKey，跟JMQ沟通申请broker和定位问题。

**事中**

> 关注上游消息入队数和下游消息出队数，保证数量差不多，同时入队TP99恒定未有大幅度升高。
>
> 若积压可提高并行数，若CPU高可暂停其他延时消费。

2.数据库CPU和机器CPU

3.JSF消费tp99太高



****

# 4.数据统计

|              | 01--06日 | 26--31日 | 环比     |
| ------------ | -------- | -------- | -------- |
| 订单总量     | 75301680 | 35309189 | 上升113% |
| 试用品匹配   | 218534   | 233573   | 下降6.4% |
| 物流广告匹配 | 4394743  | 4865286  | 下降9.6% |

![image-20201107112026206](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201107112026206.png)







**物流**

![image-20201107112603190](C:\Users\zhangkai324\AppData\Roaming\Typora\typora-user-images\image-20201107112603190.png)

## 其他

1. 黄金流程JMQ匹配 orderType

   > orderType:0    京东商城自营商品订单，或未拆分的含pop、自营混合商品的父单
   >
   > orderType:18   先款订单，京东不负责配送，厂商直送类似SOP，但在定价、结算有一定区别；
   >
   > 后款订单，销售凭证由京东或者第三方物流公司配送，收款确认后由厂家直发。（业务已经停止）
   >
   > orderType:22   Sale On POP：使用京东销售；
   >
   > orderType:112   FCS   自营采购，以销定结 

   

   